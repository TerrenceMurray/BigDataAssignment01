{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7d42dc",
   "metadata": {},
   "source": [
    "# COMP 3610: Big Data Analytics\n",
    "## Assignment 1\n",
    "\n",
    "Build an end-to-end data pipeline that ingests, transforms, and\n",
    "analyzes the NYC Yellow Taxi Trip dataset, culminating in an interactive visualization\n",
    "dashboard. This assignment integrates the skills covered in weeks 1-3 of the course: Python\n",
    "data engineering, SQL querying, and data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2eb9a3",
   "metadata": {},
   "source": [
    "### Part 1: Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a2f371",
   "metadata": {},
   "source": [
    "#### Step 1: Download Data Files\n",
    "Download `taxi_zone_lookup.csv` and `yellow_tripdata_2024-01.parquet` using the python `requests` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded taxi_zone_lookup.csv\n",
      "Successfully downloaded yellow_tripdata_2024-01.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import polars as pl\n",
    "# Using pyarrow.parquet to read only the metadata from the file\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "\n",
    "DATA_DIR: str = './data/raw'\n",
    "\n",
    "# Define the request URLs to send the GET requests to:\n",
    "YELLOW_TRIP_DATA_URL: str = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet'\n",
    "YELLOW_TRIP_FILENAME: str = 'yellow_tripdata_2024-01.parquet'\n",
    "\n",
    "TAXI_ZONE_LOOKUP_URL: str = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
    "TAXI_ZONE_FILENAME: str   = 'taxi_zone_lookup.csv'\n",
    "\n",
    "# Ensure the data directory exists\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Fetch taxi_zone_lookup.csv\n",
    "try:\n",
    "    r = requests.get(TAXI_ZONE_LOOKUP_URL)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    with open(f'{DATA_DIR}/taxi_zone_lookup.csv', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    print('Successfully downloaded taxi_zone_lookup.csv')\n",
    "except requests.RequestException as e:\n",
    "    print(f'Failed to fetch taxi_zone_lookup.csv: {e}')\n",
    "\n",
    "# Fetch yellow_tripdata_2024-01.parquet (using data streaming)\n",
    "try:\n",
    "    r = requests.get(YELLOW_TRIP_DATA_URL, stream=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    with open(f'{DATA_DIR}/yellow_tripdata_2024-01.parquet', 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "    print('Successfully downloaded yellow_tripdata_2024-01.parquet')\n",
    "except requests.RequestException as e:\n",
    "    print(f'Failed to fetch yellow_tripdata_2024-01.parquet: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac356dd",
   "metadata": {},
   "source": [
    "#### Step 2: Data Validation\n",
    "\n",
    "Validate the downloaded dataset against the expected schema below.\n",
    "\n",
    "##### Schema\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `tpep_pickup_datetime` | Timestamp when the meter was engaged |\n",
    "| `tpep_dropoff_datetime` | Timestamp when the meter was disengaged |\n",
    "| `PULocationID` | TLC Taxi Zone ID for pickup location (join with lookup table) |\n",
    "| `DOLocationID` | TLC Taxi Zone ID for dropoff location (join with lookup table) |\n",
    "| `passenger_count` | Number of passengers (driver-entered) |\n",
    "| `trip_distance` | Trip distance in miles (from taximeter) |\n",
    "| `fare_amount` | Time-and-distance fare calculated by the meter |\n",
    "| `tip_amount` | Tip amount (auto-populated for credit card payments only) |\n",
    "| `total_amount` | Total amount charged to passengers (excludes cash tips) |\n",
    "| `payment_type` | 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown |\n",
    "\n",
    "##### Requirements\n",
    "\n",
    "- Verify all expected columns exist in the dataset\n",
    "- Check that date columns are valid datetime types\n",
    "- Report total row count and print a summary to the console\n",
    "- Raise an exception or exit with an error message if validation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda37f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2964624 row/s\n",
      "Columns: 19 column/s\n",
      "Validated yellow_tripdata_2024-01.parquet successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read yellow trip parquet file\n",
    "yt_file = pq.ParquetFile(os.path.join(DATA_DIR, YELLOW_TRIP_FILENAME))\n",
    "\n",
    "# Report row and column counts\n",
    "print(f'Rows: {yt_file.metadata.num_rows} row/s\\nColumns: {yt_file.metadata.num_columns} column/s')\n",
    "\n",
    "# Verify yellow trip data schema\n",
    "try:\n",
    "    required_columns = [\n",
    "        'tpep_pickup_datetime', \n",
    "        'tpep_dropoff_datetime', \n",
    "        'PULocationID', \n",
    "        'DOLocationID', \n",
    "        'passenger_count', \n",
    "        'trip_distance',\n",
    "        'fare_amount', \n",
    "        'tip_amount', \n",
    "        'total_amount', \n",
    "        'payment_type'\n",
    "        ]\n",
    "    \n",
    "    schema = yt_file.schema_arrow\n",
    "\n",
    "    # Verify that the schema contains the column\n",
    "    for col in required_columns:\n",
    "        if col not in schema.names:\n",
    "            raise IndexError(f'column \"{col}\" could not be found in the schema')\n",
    "        \n",
    "    # Verify that the datetime columns are actually datetime types\n",
    "    for col in required_columns[:2]:\n",
    "        field = schema.field(col)\n",
    "        if not pa.types.is_timestamp(field.type):\n",
    "            raise TypeError(f'\"{col}\" is not a datetime type, got {field.type}')\n",
    "    \n",
    "    print(f'Validated {YELLOW_TRIP_FILENAME} successfully!')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Invalid schema for yellow trip data: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9d36c",
   "metadata": {},
   "source": [
    "### Part 2: Data Transformation & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9640e",
   "metadata": {},
   "source": [
    "##### Data Cleaning & Feature Engineering\n",
    "\n",
    "Using `polars`, remove all rows with null values in critical columns (`tpep_pickup_datetime`, `tpep_dropoff_datetime`, `PULocationID`, `DOLocationID`, `fare_amount`). Also invalid trips should be filtered, i.e.,\n",
    "- `trip_distance <= 0`\n",
    "- `fare_amount < 0 || fare_amount > 500.0`\n",
    "- `tpep_dropoff_datetime < tpep_pickup_datetime`\n",
    "\n",
    "Additionally, create 4 new derived columns, i.e.,\n",
    "- `trip_speed_mph` = `trip_distance` / (`trip_duration_minutes` / 60)\n",
    "- `pickup_hour` = `tpep_pickup_datetime`.hour\n",
    "- `pickup_day_of_week` = `tpep_pickup_datetime`.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c1b7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows:              2964624\n",
      "Total rows removed:        94522\n",
      "Remaining rows:            2870102\n"
     ]
    }
   ],
   "source": [
    "df = pl.scan_parquet(os.path.join(DATA_DIR, YELLOW_TRIP_FILENAME))\n",
    "initial_row_count = yt_file.metadata.num_rows\n",
    "\n",
    "# Filter out invalid trips\n",
    "df = df.filter(\n",
    "    ~(\n",
    "        (pl.col('trip_distance') <= 0) |\n",
    "        (pl.col('fare_amount') < 0) |\n",
    "        (pl.col('fare_amount') > 500.0) |\n",
    "        (pl.col('tpep_dropoff_datetime') < pl.col('tpep_pickup_datetime'))\n",
    "    )\n",
    ").with_columns(\n",
    "    (pl.col('tpep_dropoff_datetime') - pl.col('tpep_pickup_datetime')).dt.total_minutes().alias('trip_duration_minutes'),\n",
    ").with_columns(\n",
    "    (pl.col('trip_distance') / (pl.col('trip_duration_minutes') / 60)).alias('trip_speed_mph'),\n",
    "    pl.col('tpep_pickup_datetime').dt.hour().alias('pickup_hour'),\n",
    "    pl.col('tpep_pickup_datetime').dt.to_string('%A').alias('pickup_day_of_week'),\n",
    ")\n",
    "\n",
    "df = df.collect()\n",
    "final_row_count = df.shape[0]\n",
    "\n",
    "print(f'Initial rows:              {initial_row_count}')\n",
    "print(f'Total rows removed:        {initial_row_count - final_row_count}')\n",
    "print(f'Remaining rows:            {final_row_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5b6e2",
   "metadata": {},
   "source": [
    "##### 1. What are the top 10 busiest pickup zones by total number of trips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a4d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────────────────────┬────────────┐\n",
       "│             zone             │ trip_count │\n",
       "│           varchar            │   int64    │\n",
       "├──────────────────────────────┼────────────┤\n",
       "│ Midtown Center               │     140161 │\n",
       "│ Upper East Side South        │     140134 │\n",
       "│ JFK Airport                  │     138478 │\n",
       "│ Upper East Side North        │     133975 │\n",
       "│ Midtown East                 │     104356 │\n",
       "│ Times Sq/Theatre District    │     102972 │\n",
       "│ Penn Station/Madison Sq West │     102161 │\n",
       "│ Lincoln Square East          │     101800 │\n",
       "│ LaGuardia Airport            │      87715 │\n",
       "│ Upper West Side South        │      86475 │\n",
       "├──────────────────────────────┴────────────┤\n",
       "│ 10 rows                         2 columns │\n",
       "└───────────────────────────────────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zones = pl.read_csv(os.path.join(DATA_DIR, TAXI_ZONE_FILENAME))\n",
    "\n",
    "duckdb.sql(\"\"\"\n",
    "    SELECT z.Zone as zone, COUNT(*) as trip_count \n",
    "    FROM df \n",
    "    JOIN zones z ON df.PULocationID = z.LocationID\n",
    "    GROUP BY PULocationID, z.Zone \n",
    "    ORDER BY trip_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b917c41",
   "metadata": {},
   "source": [
    "The top 10 pick up zone are `Midtown Center`, `Upper East Side South`, `JFK Airport`, `Upper East Side North`, `Midtown East`, `Times Sq/Theatre District`, `Pen Station/Madison Sq West`, `Lincoln Square East`, `LaGuardia Airport`, and `Upper West Side South`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f8c20a",
   "metadata": {},
   "source": [
    "##### 2. What is the average fare amount for each hour of the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "639b3874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────────────┬─────────────┐\n",
       "│ average_fare_amount │ pickup_hour │\n",
       "│       varchar       │    int8     │\n",
       "├─────────────────────┼─────────────┤\n",
       "│ $19.68              │           0 │\n",
       "│ $17.73              │           1 │\n",
       "│ $16.62              │           2 │\n",
       "│ $18.53              │           3 │\n",
       "│ $23.44              │           4 │\n",
       "│ $27.49              │           5 │\n",
       "│ $22.03              │           6 │\n",
       "│ $18.75              │           7 │\n",
       "│ $17.82              │           8 │\n",
       "│ $17.94              │           9 │\n",
       "│   ·                 │           · │\n",
       "│   ·                 │           · │\n",
       "│   ·                 │           · │\n",
       "│ $19.27              │          14 │\n",
       "│ $19.11              │          15 │\n",
       "│ $19.46              │          16 │\n",
       "│ $18.12              │          17 │\n",
       "│ $17.01              │          18 │\n",
       "│ $17.63              │          19 │\n",
       "│ $18.05              │          20 │\n",
       "│ $18.29              │          21 │\n",
       "│ $19.11              │          22 │\n",
       "│ $20.24              │          23 │\n",
       "├─────────────────────┴─────────────┤\n",
       "│ 24 rows (20 shown)      2 columns │\n",
       "└───────────────────────────────────┘"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "    SELECT PRINTF('$%.2f', AVG(CAST(fare_amount AS float))) as average_fare_amount, pickup_hour\n",
    "    FROM df\n",
    "    GROUP BY pickup_hour\n",
    "    ORDER BY pickup_hour ASC\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
